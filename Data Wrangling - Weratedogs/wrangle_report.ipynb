{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following report I will be jotting down the wrangling efforts undertaken,\n",
    "\n",
    "The Data wrangling done here consists of the following steps:\n",
    "- Gathering data\n",
    "- Assessing data\n",
    "- Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "\n",
    "The Data Gathering was done on three sources of data in which each one had its own complexity and method of data extraction.\n",
    "\n",
    "The following data were gathered in this Udacity project.\n",
    "\n",
    "- The WeRateDogs Twitter archive. This dataset consists of each tweet's physical data such as ids, text, timestamp,..etc and also some imputations done on the physical data was added such as dog stage (puppo, doggo,..). This data was given by udacity as twitter_archive_enhanced.csv. The file can be downloaded from Jupyter notebook.\n",
    "\n",
    "- The tweet image predictions data, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) was hosted on Udacity's servers and was downloaded programmatically using the **Requests library** from the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv.\n",
    "\n",
    "- Each tweet's retweet count and favorite (\"like\") count data. Using the tweet IDs in the WeRateDogs Twitter archive, I was required to query the Twitter API for each tweet's JSON data using **Python's Tweepy library** and store each tweet's entire set of JSON data in a file called tweet_json.txt file as a list of jsons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Gathered Files\n",
    "- The WeRateDogs Twitter archive was loaded through pandas read_csv function.\n",
    "- the tweet image predictions data was also loaded through pandas read_csv function but since this is a tab separated file(.tsv) we use separator parameter in read_csv function to specify to read the file as tab separated.\n",
    "- The json data gathered through Twitter API was loaded to a file as a list of jsons. Then the file was read back again into a pandas dataframe. **Python Library json** was predominantly used in this process and writing and reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "\n",
    "Data assessment was predominantly done through visual and programmatic assessments.\n",
    "\n",
    "- **Visual Assessments :** The data gathered was loaded into a spreadsheet program (Excel in this case) and was visually skimmed for data quality issues. \n",
    "- **Programmatic Assessments :** This was done predominantly through Pandas methods info(), describe(), sample(), head(), tail() and also by filtering the data.\n",
    "\n",
    "Following Issues were Identified:\n",
    "\n",
    "#### Quality Issues\n",
    "*These are mainly issues related with content of the data*\n",
    "##### `twitter_archive_enhanced` Table\n",
    "- tweet_id, in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id are in int format not in string.\n",
    "- timestamp column is not in datetime format.\n",
    "- rating columns are in int and not in float.\n",
    "- name values incorrectly represented as 'None' instead of Nan.\n",
    "- dog stage columns Nans incorrectly represented as None.\n",
    "- Ratings values are incorrect in few observations.\n",
    "- Rows retweeted with 'RT @dog_rates' are duplicated data.\n",
    "- name value 'a' is repeated many times incorrectly\n",
    "\n",
    "##### `image_predictions` Table\n",
    "- tweet_id is int format not in string.\n",
    "- Dog type column values are not consistent and have underscores.\n",
    "\n",
    "#### Tidiness\n",
    "*These are issues related with structure of the data*\n",
    "##### `twitter_archive_enhanced` Table\n",
    "- Dog stage - to be converted to one column from four columns (doggo, pupper,..).\n",
    "- Text column to be separated into text and tweet link.\n",
    "- Replace retweet columns with a single column which has only two values - retweeted or not.\n",
    "- Replace user replied info columns with a single column which has only two values - user replied tweet or not.\n",
    "- Strip source columns to exclude html tags.\n",
    "\n",
    "##### `image_predictions` Table\n",
    "- Filter predicted dog breeds for each observations to single columns (p1, p1_dog, p1_conf, p2, p2_dog,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "In data cleaning, for each issue identified during our assessment of the data we defined three things,\n",
    "- **Define the issue :** Here we defined the issue and how to tackle this issue on a high level.\n",
    "- **Code :** In this part we manually or programmatically code to solve our data issue.\n",
    "- **Test :** After running our code on the data issue here we test if our code was successful in resolving the errors.\n",
    "\n",
    "While cleaning the dataset we resorted to mostly cleaning the data programmatically rather than manually since for each error identified there was more than one occurrence and resorting to manual cleaning for these errors would be tedious. Most of the major errors were identified when resolving each of the predefined issues during assessments hence we were reiterating back and forth between assessment and cleaning during this process.\n",
    "\n",
    "We were also reiterating back to assessment and cleaning our data while analyzing our dataset during our analyses and visualization stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I finally arrived at three datasets after which I merged all three datasets to form a master dataset. \n",
    "\n",
    "I believe that Cleaning a dataset is a continous process and can sometimes be endless since there is no definition for a perfect dataset. I have done my best here to arrive at a place where i have no issues while analyzing my cleaned dataset. Eventhough Udacity requests for around 8 quality issues and 2 tidiness issues i had to go beyond this so that i am not being limited in my analyses and the insights that i have in mind for this dataset.\n",
    "\n",
    "Wrangling is the most time consuming part of data scientist's job, but when done right your results from your models and insights on the data becomes highly valuable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
